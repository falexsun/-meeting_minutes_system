# =============================================================================
# Конфигурация Meminto для локального сервера с Ollama
# =============================================================================

# -----------------------------------------------------------------------------
# Hugging Face Access Token (обязательно)
# Необходим для загрузки моделей диаризации (разделения по спикерам)
# Как получить токен:
# 1. Зарегистрируйтесь на https://huggingface.co/
# 2. Перейдите на https://huggingface.co/pyannote/speaker-diarization
# 3. Примите условия использования
# 4. Получите токен на https://huggingface.co/settings/tokens
# -----------------------------------------------------------------------------
HUGGING_FACE_ACCESS_TOKEN=your_hugging_face_token_here

# -----------------------------------------------------------------------------
# Настройки Ollama (для локальной LLM)
# Используйте эти настройки, если запускаете программу с флагом --use-ollama
# -----------------------------------------------------------------------------
# Модель Ollama для генерации протоколов
# Рекомендуемые модели для русского языка:
# - llama3.2 (быстрая, хорошо работает с русским)
# - qwen2.5:7b (отличная поддержка русского языка)
# - mistral (хорошая альтернатива)
# - gemma2:9b (хорошая производительность)
OLLAMA_MODEL=qwen2.5:7b

# URL Ollama API (обычно локальный сервер)
OLLAMA_URL=http://localhost:11434/api/chat

# Максимальное количество токенов для генерации
# Для совещаний длительностью 1-2 часа рекомендуется 4000-8000
OLLAMA_MAX_TOKENS=8000

# -----------------------------------------------------------------------------
# Настройки удаленного транскрайбера (опционально)
# Используйте, если хотите использовать удаленный сервер для транскрипции
# вместо локального Whisper
# -----------------------------------------------------------------------------
# TRANSCRIBER_URL=http://your-transcriber-server/v1/audio/transcriptions
# TRANSCRIBER_AUTHORIZATION=Bearer your_api_key_here

# -----------------------------------------------------------------------------
# Настройки для использования внешних LLM API (альтернатива Ollama)
# Используйте эти настройки, если НЕ используете флаг --use-ollama
# -----------------------------------------------------------------------------
# Для OpenAI:
# LLM_URL=https://api.openai.com/v1/chat/completions
# LLM_MODEL=gpt-4
# LLM_MAX_TOKENS=4000
# LLM_AUTHORIZATION=Bearer your_openai_api_key

# Для Azure OpenAI:
# LLM_URL=https://your-resource.openai.azure.com/openai/deployments/your-deployment/chat/completions?api-version=2023-05-15
# LLM_MODEL=gpt-4
# LLM_MAX_TOKENS=4000
# LLM_AUTHORIZATION=your_azure_api_key

# Для локального сервера OpenAI-совместимого API:
# LLM_URL=http://localhost:8000/v1/chat/completions
# LLM_MODEL=your_model_name
# LLM_MAX_TOKENS=4000
# LLM_AUTHORIZATION=Bearer your_api_key
