# Meminto - Инструмент для создания протоколов совещаний на основе ИИ

Meminto - это инструмент на основе ИИ для создания протоколов совещаний. Просто передайте программе аудиофайл `.wav` записанного совещания, и она автоматически создаст протокол.

**Основные возможности:**
- **Диаризация спикеров** - использует pyannote.audio для разделения речи разных участников
- **Транскрипция** - использует Whisper для преобразования речи в текст
- **Генерация протокола** - использует локальную LLM (через Ollama) для создания структурированного протокола

**Адаптировано для русскоязычных совещаний:**
- Полная поддержка русского языка
- Локальная обработка через Ollama (без передачи данных третьим лицам)
- Оптимизировано для длинных совещаний (1-2 часа)

## Быстрая установка и запуск

### Шаг 1. Клонирование репозитория
```bash
git clone https://github.com/FlorianSchepers/Meminto.git
cd Meminto
```

### Шаг 2. Установка зависимостей
```bash
pipx install poetry
poetry install
poetry shell
```

### Шаг 3. Установка и запуск Ollama

**Для Windows:**
1. Скачайте Ollama с https://ollama.com/download
2. Установите Ollama
3. Откройте командную строку и запустите:
```bash
ollama pull qwen2.5:7b
```

**Для Linux/macOS:**
```bash
curl -fsSL https://ollama.com/install.sh | sh
ollama pull qwen2.5:7b
```

**Рекомендуемые модели для русского языка:**
- `qwen2.5:7b` - отличная поддержка русского языка (рекомендуется)
- `llama3.2` - быстрая, хорошо работает с русским
- `mistral` - хорошая альтернатива
- `gemma2:9b` - высокая производительность

### Шаг 4. Настройка переменных окружения

Создайте файл `.env` в корневой папке проекта:
```bash
cp .env.example .env
```

Отредактируйте `.env` и заполните следующие поля:
```bash
# Обязательно: токен Hugging Face для диаризации
HUGGING_FACE_ACCESS_TOKEN=ваш_токен_hugging_face

# Настройки Ollama
OLLAMA_MODEL=qwen2.5:7b
OLLAMA_URL=http://localhost:11434/api/chat
OLLAMA_MAX_TOKENS=8000
```

**Как получить токен Hugging Face:**
1. Зарегистрируйтесь на https://huggingface.co/
2. Перейдите на https://huggingface.co/pyannote/speaker-diarization
3. Примите условия использования модели
4. Получите токен на https://huggingface.co/settings/tokens

### Шаг 5. Запуск Meminto

```bash
# С использованием Ollama (локальная LLM)
python meminto/main.py -f путь/к/аудио.wav -l russian --use-ollama

# С указанием папки для результатов
python meminto/main.py -f путь/к/аудио.wav -o output -l russian --use-ollama
```

## Подробное описание

### Параметры командной строки

```bash
python meminto/main.py [OPTIONS]
```

**Опции:**
- `-f, --input-file` - Путь к аудиофайлу (.wav или .mp3)
- `-o, --output-folder` - Папка для сохранения результатов (по умолчанию: `output`)
- `-l, --language` - Язык протокола: `russian`, `english`, `german` (по умолчанию: `russian`)
- `--use-ollama` - Использовать Ollama для локальной LLM (рекомендуется)
- `--remote-transcriber` - Использовать удаленный сервер для транскрипции

**Примеры:**

```bash
# Обработка совещания на русском языке с Ollama
python meminto/main.py -f meeting.wav -l russian --use-ollama

# Обработка с удаленным транскрайбером и Ollama
python meminto/main.py -f meeting.wav --remote-transcriber --use-ollama

# Использование примера из репозитория (на английском)
python meminto/main.py -f examples/Scoreboard.wav -l english --use-ollama
```

### Выходные файлы

После обработки в папке `output` будут созданы следующие файлы:

1. **diarization.txt** - Разделение по спикерам (кто и когда говорил)
2. **transcript.txt** - Полная расшифровка совещания с временными метками
3. **meeting_minutes.txt** - Структурированный протокол совещания с:
   - Целями совещания
   - Принятыми решениями
   - Назначенными задачами
   - Дополнительными заметками

### Структура протокола

Протокол содержит следующие разделы:

```markdown
**Цели:**
- Список целей совещания

**Решения:**
- Все принятые решения

**Назначенные задачи:**
- SPEAKER_01: Описание задачи
- SPEAKER_02: Описание задачи

**Дополнительные заметки:**
- Важные моменты обсуждения
```

## Системные требования

### Минимальные требования:
- Python 3.11 или выше
- 8 GB RAM
- 10 GB свободного места на диске

### Рекомендуемые требования для совещаний 1-2 часа:
- Python 3.11+
- 16 GB RAM
- NVIDIA GPU с 8+ GB VRAM (для ускорения транскрипции)
- 20 GB свободного места на диске
- SSD для быстрой работы

### Поддержка GPU

Программа автоматически использует GPU, если он доступен:
- **Для транскрипции (Whisper):** NVIDIA GPU с CUDA
- **Для Ollama:** автоматическое определение (CUDA, ROCm, Metal для Mac)

## Оптимизация для длинных совещаний

Для совещаний длительностью 1-2 часа рекомендуется:

1. **Увеличить OLLAMA_MAX_TOKENS:**
   ```bash
   OLLAMA_MAX_TOKENS=8000
   ```

2. **Использовать GPU для Ollama:**
   ```bash
   # Ollama автоматически использует GPU, если доступен
   # Проверить можно командой:
   ollama list
   ```

3. **Использовать более мощную модель:**
   ```bash
   # Для лучшего качества на русском:
   ollama pull qwen2.5:14b

   # В .env:
   OLLAMA_MODEL=qwen2.5:14b
   ```

4. **Предварительная обработка аудио:**
   - Используйте формат .wav для лучшей совместимости
   - Частота дискретизации 16 kHz оптимальна
   - Моно-канал предпочтительнее стерео

## Решение проблем

### Ollama не запускается
```bash
# Проверьте, запущен ли Ollama
curl http://localhost:11434/api/tags

# Перезапустите Ollama (Windows)
# Закройте Ollama через трей и запустите снова

# Перезапустите Ollama (Linux/macOS)
systemctl restart ollama
```

### Ошибка "CUDA out of memory"
```bash
# Используйте модель меньшего размера
ollama pull qwen2.5:3b

# В .env:
OLLAMA_MODEL=qwen2.5:3b
```

### Медленная обработка
1. Убедитесь, что используется GPU
2. Используйте модель меньшего размера
3. Разбейте аудио на части по 30-40 минут

### Плохое качество транскрипции на русском
1. Убедитесь, что используется модель Whisper, поддерживающая русский:
   - `medium` или `large` (по умолчанию в коде)
2. Проверьте качество аудио (минимум шумов, четкая речь)

## Альтернативные конфигурации

### Использование внешнего API вместо Ollama

Если вы хотите использовать внешний API (OpenAI, Azure, и т.д.):

```bash
# В .env:
LLM_URL=https://api.openai.com/v1/chat/completions
LLM_MODEL=gpt-4
LLM_MAX_TOKENS=4000
LLM_AUTHORIZATION=Bearer ваш_api_ключ

# Запуск БЕЗ флага --use-ollama:
python meminto/main.py -f meeting.wav -l russian
```

### Использование удаленного транскрайбера

Для очень длинных совещаний можно использовать удаленный сервер:

```bash
# В .env:
TRANSCRIBER_URL=http://ваш-сервер/v1/audio/transcriptions
TRANSCRIBER_AUTHORIZATION=Bearer ваш_api_ключ

# Запуск:
python meminto/main.py -f meeting.wav --remote-transcriber --use-ollama
```

## Безопасность и конфиденциальность

**Локальная обработка:**
- Диаризация: выполняется локально через pyannote.audio
- Транскрипция: выполняется локально через Whisper
- Генерация протокола: выполняется локально через Ollama

**Все данные остаются на вашем сервере!**

Единственное подключение к интернету требуется для:
- Загрузки моделей при первом запуске (Hugging Face)
- Загрузки Ollama моделей (однократно)

После загрузки всех моделей программа может работать полностью offline.

## Лицензия

Этот проект распространяется под лицензией MIT.

## Благодарности

Проект создан на основе:
- [Meminto](https://github.com/FlorianSchepers/Meminto) by Florian Schepers
- [pyannote.audio](https://github.com/pyannote/pyannote-audio) для диаризации
- [Whisper](https://github.com/openai/whisper) для транскрипции
- [Ollama](https://ollama.com/) для локальных LLM

Адаптация для русского языка и Ollama выполнена для локальной обработки совещаний без передачи данных третьим лицам.
