import requests
import time
from typing import Optional


class LMStudioLLM:
    """
    –ö–ª–∞—Å—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –ª–æ–∫–∞–ª—å–Ω–æ–π LLM —á–µ—Ä–µ–∑ LM Studio.
    LM Studio –∏—Å–ø–æ–ª—å–∑—É–µ—Ç OpenAI-—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π API –Ω–∞ http://localhost:1234
    """

    def __init__(
        self,
        model: str,
        url: str = "http://localhost:1234/v1/chat/completions",
        temperature: float = 0.5,
        max_tokens: int = 8000,
    ):
        self.model = model
        self.url = url
        self.temperature = temperature
        self.max_tokens = max_tokens

    def infer(self, system_prompt: str, user_prompt: str) -> str:
        """
        –û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –∑–∞–ø—Ä–æ—Å –∫ LM Studio –∏ –ø–æ–ª—É—á–∞–µ—Ç –æ—Ç–≤–µ—Ç.

        Args:
            system_prompt: –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç (–∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –º–æ–¥–µ–ª–∏)
            user_prompt: –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –∑–∞–ø—Ä–æ—Å

        Returns:
            –û—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏ –≤ –≤–∏–¥–µ —Å—Ç—Ä–æ–∫–∏
        """
        headers = self._create_headers()
        parameters = self._create_parameters(system_prompt, user_prompt)

        print(f"Url –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–π –¥–ª—è LLM –∑–∞–ø—Ä–æ—Å–∞: {self.url}")
        print(f"–ú–æ–¥–µ–ª—å: {self.model}")
        print(f"–†–∞–∑–º–µ—Ä –ø—Ä–æ–º–ø—Ç–∞: {len(system_prompt) + len(user_prompt)} —Å–∏–º–≤–æ–ª–æ–≤")
        print(f"–ü–∞—Ä–∞–º–µ—Ç—Ä—ã: temperature={self.temperature}, max_tokens={self.max_tokens}")

        # Retry –ª–æ–≥–∏–∫–∞ –¥–ª—è —Å–ª—É—á–∞–µ–≤, –∫–æ–≥–¥–∞ LM Studio –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è
        max_retries = 5
        retry_delay = 10  # —Å–µ–∫—É–Ω–¥—ã

        for attempt in range(max_retries):
            try:
                if attempt > 0:
                    print(f"‚è≥ –ü–æ–ø—ã—Ç–∫–∞ {attempt + 1}/{max_retries}...")
                else:
                    print(f"ü§ñ –û—Ç–ø—Ä–∞–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –≤ LM Studio (—Ç–∞–π–º–∞—É—Ç: 60 –º–∏–Ω)...")
                
                # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Ç–∞–π–º–∞—É—Ç –¥–æ 3600 —Å–µ–∫—É–Ω–¥ (60 –º–∏–Ω—É—Ç) –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
                response = requests.post(url=self.url, headers=headers, json=parameters, timeout=3600)
                response.raise_for_status()

                response_data = response.json()

                # –î–ª—è LM Studio API —Ñ–æ—Ä–º–∞—Ç OpenAI: {"choices": [{"message": {"content": "—Ç–µ–∫—Å—Ç"}}]}
                if "choices" in response_data and len(response_data["choices"]) > 0:
                    print("‚úÖ –û—Ç–≤–µ—Ç –ø–æ–ª—É—á–µ–Ω —É—Å–ø–µ—à–Ω–æ")
                    return response_data["choices"][0]["message"]["content"]
                else:
                    raise ValueError(f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞ –æ—Ç LM Studio: {response_data}")

            except requests.exceptions.HTTPError as e:
                # –ï—Å–ª–∏ 502 Bad Gateway - –ø–æ–≤—Ç–æ—Ä—è–µ–º –ø–æ–ø—ã—Ç–∫—É
                if e.response is not None and e.response.status_code == 502:
                    if attempt < max_retries - 1:
                        print(f"‚ö†Ô∏è  –ü–æ–ª—É—á–µ–Ω–∞ –æ—à–∏–±–∫–∞ 502 (LM Studio –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è). –ü–æ–ø—ã—Ç–∫–∞ {attempt + 1}/{max_retries}. –û–∂–∏–¥–∞–Ω–∏–µ {retry_delay} —Å–µ–∫...")
                        time.sleep(retry_delay)
                        continue
                    else:
                        print(f"‚ùå –û—à–∏–±–∫–∞ 502 –ø–æ—Å–ª–µ {max_retries} –ø–æ–ø—ã—Ç–æ–∫")
                        print("üí° –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ LM Studio –∑–∞–ø—É—â–µ–Ω –∏ –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
                
                # –î–ª—è –¥—Ä—É–≥–∏—Ö –æ—à–∏–±–æ–∫ –∏–ª–∏ –µ—Å–ª–∏ –∏—Å—á–µ—Ä–ø–∞–Ω—ã –ø–æ–ø—ã—Ç–∫–∏ - –≤—ã–≤–æ–¥–∏–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
                print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞—â–µ–Ω–∏–∏ –∫ LM Studio: {e}")
                if hasattr(e, 'response') and e.response is not None:
                    print(f"–ö–æ–¥ –æ—Ç–≤–µ—Ç–∞: {e.response.status_code}")
                    print(f"–¢–µ–∫—Å—Ç –æ—Ç–≤–µ—Ç–∞: {e.response.text[:500]}")
                raise
            
            except requests.exceptions.RequestException as e:
                print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞—â–µ–Ω–∏–∏ –∫ LM Studio: {e}")
                raise
        
        # –ï—Å–ª–∏ –¥–æ—à–ª–∏ —Å—é–¥–∞ - –≤—Å–µ –ø–æ–ø—ã—Ç–∫–∏ –ø—Ä–æ–≤–∞–ª–∏–ª–∏—Å—å
        raise RuntimeError(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –æ—Ç–≤–µ—Ç –æ—Ç LM Studio –ø–æ—Å–ª–µ {max_retries} –ø–æ–ø—ã—Ç–æ–∫")

    def _create_headers(self) -> dict:
        """–°–æ–∑–¥–∞–µ—Ç –∑–∞–≥–æ–ª–æ–≤–∫–∏ –¥–ª—è HTTP –∑–∞–ø—Ä–æ—Å–∞"""
        headers = {
            "Content-Type": "application/json"
        }
        return headers

    def _create_parameters(
        self, system_prompt: str, user_prompt: str
    ) -> dict:
        """
        –°–æ–∑–¥–∞–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞ –∫ LM Studio API.
        
        –§–æ—Ä–º–∞—Ç OpenAI-—Å–æ–≤–º–µ—Å—Ç–∏–º–æ–≥–æ API:
        {
          "model": "model_name",
          "messages": [
            {"role": "system", "content": "..."},
            {"role": "user", "content": "..."}
          ],
          "temperature": 0.5,
          "max_tokens": 4000
        }
        """
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ]

        parameters = {
            "model": self.model,
            "messages": messages,
            "temperature": self.temperature,
            "max_tokens": self.max_tokens,
        }

        return parameters
